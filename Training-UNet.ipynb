{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, BatchNormalization\n",
    "from keras.layers import Conv2D\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "from scipy import io\n",
    "import h5py\n",
    "import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train_X = np.load('train_longi_X_v1.0.npy')\n",
    "# train_Y = np.load('train_longi_Y_v1.0.npy')\n",
    "\n",
    "train_X = np.load('train_trans_X_v1.0.npy')\n",
    "train_Y = np.load('train_trans_Y_v1.0.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 100\n",
    "fine_tuning = False\n",
    "load_model_name = 'D://Models2/trans_v1.0.h5'\n",
    "epoch_to_start = 0\n",
    "\n",
    "save_temp_model_name = 'D://Models/temp.h'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'D://Models2/trans_v1.0.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3000, 256, 256, 1)\n",
      "3000 train samples\n"
     ]
    }
   ],
   "source": [
    "## The data, shuffled and split between train and test sets:\n",
    "print('x_train shape:', train_X.shape)\n",
    "print(train_X.shape[0], 'train samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fresh\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input (InputLayer)               (None, None, None, 1) 0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1_1 (Conv2D)                 (None, None, None, 64 640         input[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, None, None, 64 256         conv1_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, None, None, 64 0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_2 (Conv2D)                 (None, None, None, 64 36928       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, None, None, 64 256         conv1_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, None, None, 64 0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv1_3 (Conv2D)                 (None, None, None, 64 36928       activation_2[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)   (None, None, None, 64 0           conv1_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, None, None, 64 256         max_pooling2d_1[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, None, None, 64 0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_1 (Conv2D)                 (None, None, None, 12 73856       activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNorm (None, None, None, 12 512         conv2_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, None, None, 12 0           batch_normalization_4[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv2_2 (Conv2D)                 (None, None, None, 12 147584      activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)   (None, None, None, 12 0           conv2_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNorm (None, None, None, 12 512         max_pooling2d_2[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, None, None, 12 0           batch_normalization_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv3_1 (Conv2D)                 (None, None, None, 25 295168      activation_5[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNorm (None, None, None, 25 1024        conv3_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, None, None, 25 0           batch_normalization_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv3_2 (Conv2D)                 (None, None, None, 25 590080      activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)   (None, None, None, 25 0           conv3_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNorm (None, None, None, 25 1024        max_pooling2d_3[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_7 (Activation)        (None, None, None, 25 0           batch_normalization_7[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv4_1 (Conv2D)                 (None, None, None, 51 1180160     activation_7[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNorm (None, None, None, 51 2048        conv4_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_8 (Activation)        (None, None, None, 51 0           batch_normalization_8[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv4_2 (Conv2D)                 (None, None, None, 51 2359808     activation_8[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)   (None, None, None, 51 0           conv4_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNorm (None, None, None, 51 2048        max_pooling2d_4[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "activation_9 (Activation)        (None, None, None, 51 0           batch_normalization_9[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "conv5_1 (Conv2D)                 (None, None, None, 10 4719616     activation_9[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNor (None, None, None, 10 4096        conv5_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_10 (Activation)       (None, None, None, 10 0           batch_normalization_10[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv5_2 (Conv2D)                 (None, None, None, 51 4719104     activation_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNor (None, None, None, 51 2048        conv5_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_11 (Activation)       (None, None, None, 51 0           batch_normalization_11[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2D)   (None, None, None, 51 0           activation_11[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, None, None, 10 0           up_sampling2d_1[0][0]            \n",
      "                                                                   conv4_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNor (None, None, None, 10 4096        concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_12 (Activation)       (None, None, None, 10 0           batch_normalization_12[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_1 (Conv2D)                 (None, None, None, 51 4719104     activation_12[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNor (None, None, None, 51 2048        conv6_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_13 (Activation)       (None, None, None, 51 0           batch_normalization_13[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv6_2 (Conv2D)                 (None, None, None, 25 1179904     activation_13[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNor (None, None, None, 25 1024        conv6_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_14 (Activation)       (None, None, None, 25 0           batch_normalization_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2D)   (None, None, None, 25 0           activation_14[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, None, None, 51 0           up_sampling2d_2[0][0]            \n",
      "                                                                   conv3_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNor (None, None, None, 51 2048        concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_15 (Activation)       (None, None, None, 51 0           batch_normalization_15[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv7_1 (Conv2D)                 (None, None, None, 25 1179904     activation_15[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNor (None, None, None, 25 1024        conv7_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_16 (Activation)       (None, None, None, 25 0           batch_normalization_16[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv7_2 (Conv2D)                 (None, None, None, 12 295040      activation_16[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNor (None, None, None, 12 512         conv7_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_17 (Activation)       (None, None, None, 12 0           batch_normalization_17[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_3 (UpSampling2D)   (None, None, None, 12 0           activation_17[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)      (None, None, None, 25 0           up_sampling2d_3[0][0]            \n",
      "                                                                   conv2_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNor (None, None, None, 25 1024        concatenate_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_18 (Activation)       (None, None, None, 25 0           batch_normalization_18[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv8_1 (Conv2D)                 (None, None, None, 12 295040      activation_18[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNor (None, None, None, 12 512         conv8_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_19 (Activation)       (None, None, None, 12 0           batch_normalization_19[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv8_2 (Conv2D)                 (None, None, None, 64 73792       activation_19[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNor (None, None, None, 64 256         conv8_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_20 (Activation)       (None, None, None, 64 0           batch_normalization_20[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)   (None, None, None, 64 0           activation_20[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)      (None, None, None, 12 0           up_sampling2d_4[0][0]            \n",
      "                                                                   conv1_3[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNor (None, None, None, 12 512         concatenate_4[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_21 (Activation)       (None, None, None, 12 0           batch_normalization_21[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv9_1 (Conv2D)                 (None, None, None, 64 73792       activation_21[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNor (None, None, None, 64 256         conv9_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_22 (Activation)       (None, None, None, 64 0           batch_normalization_22[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv9_2 (Conv2D)                 (None, None, None, 64 36928       activation_22[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNor (None, None, None, 64 256         conv9_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "activation_23 (Activation)       (None, None, None, 64 0           batch_normalization_23[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "conv9_3 (Conv2D)                 (None, None, None, 1) 65          activation_23[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "output (Add)                     (None, None, None, 1) 0           conv9_3[0][0]                    \n",
      "                                                                   input[0][0]                      \n",
      "====================================================================================================\n",
      "Total params: 22,041,089\n",
      "Trainable params: 22,027,265\n",
      "Non-trainable params: 13,824\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if fine_tuning:\n",
    "    print('Loading the saved model')\n",
    "    model = load_model(load_model_name)\n",
    "    model.summary()\n",
    "\n",
    "else:\n",
    "    print('Start fresh')\n",
    "    \n",
    "    model = models.UNet24(64)\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.01)\n",
    "\n",
    "    model.compile(loss='mean_absolute_error', optimizer=opt)\n",
    "#     model.compile(loss='mean_squared_error', optimizer=opt)\n",
    "\n",
    "#     json_string = model.to_json()\n",
    "#     open('FCNmodel.json','w').write(json_string)\n",
    "#     model.save('FCNmodel.h5',overwrite=True)\n",
    "    \n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "3000/3000 [==============================] - 310s - loss: 0.0096   \n",
      "Epoch 2/100\n",
      "3000/3000 [==============================] - 303s - loss: 0.0066   \n",
      "Epoch 3/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0065   \n",
      "Epoch 4/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0064   \n",
      "Epoch 5/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0064   \n",
      "Epoch 6/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0063   \n",
      "Epoch 7/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0063   \n",
      "Epoch 8/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0063   \n",
      "Epoch 9/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0063   \n",
      "Epoch 10/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0063   \n",
      "Epoch 11/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0062   \n",
      "Epoch 12/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0062   \n",
      "Epoch 13/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0061   \n",
      "Epoch 14/100\n",
      "2999/3000 [============================>.] - ETA: 0s - loss: 0.0061\n",
      "Epoch 00013: reducing learning rate to 0.004999999888241291.\n",
      "3000/3000 [==============================] - 296s - loss: 0.0061   \n",
      "Epoch 15/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0059   \n",
      "Epoch 16/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0058   \n",
      "Epoch 17/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0057   \n",
      "Epoch 18/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0056   \n",
      "Epoch 19/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0055   \n",
      "Epoch 20/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0054   \n",
      "Epoch 21/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0053   \n",
      "Epoch 22/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0052   \n",
      "Epoch 23/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0051   \n",
      "Epoch 24/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0050   \n",
      "Epoch 25/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0050   - ETA: 2s - loss:  - ETA: 1\n",
      "Epoch 26/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0049   \n",
      "Epoch 27/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0048   \n",
      "Epoch 28/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0048   \n",
      "Epoch 29/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0047   \n",
      "Epoch 30/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0046   \n",
      "Epoch 31/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0046   \n",
      "Epoch 32/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0045   \n",
      "Epoch 33/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0045   \n",
      "Epoch 34/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0044   \n",
      "Epoch 35/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0044   \n",
      "Epoch 36/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0043   \n",
      "Epoch 37/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0043   \n",
      "Epoch 38/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0042   \n",
      "Epoch 39/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0042   \n",
      "Epoch 40/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0041   \n",
      "Epoch 41/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0041   \n",
      "Epoch 42/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0041   \n",
      "Epoch 43/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0040   \n",
      "Epoch 44/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0040   \n",
      "Epoch 45/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0040   \n",
      "Epoch 46/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0039   \n",
      "Epoch 47/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0039   \n",
      "Epoch 48/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0039   \n",
      "Epoch 49/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0038   \n",
      "Epoch 50/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0038   \n",
      "Epoch 51/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0038   \n",
      "Epoch 52/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0038   \n",
      "Epoch 53/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0037   \n",
      "Epoch 54/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0037   \n",
      "Epoch 55/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0037   \n",
      "Epoch 56/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0037   \n",
      "Epoch 57/100\n",
      "3000/3000 [==============================] - 296s - loss: 0.0036   \n",
      "Epoch 58/100\n",
      "3000/3000 [==============================] - 296s - loss: 0.0036   \n",
      "Epoch 59/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0036   \n",
      "Epoch 60/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0036   \n",
      "Epoch 61/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0036   \n",
      "Epoch 62/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0035   \n",
      "Epoch 63/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0035   \n",
      "Epoch 64/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0035   \n",
      "Epoch 65/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0035   \n",
      "Epoch 66/100\n",
      "2999/3000 [============================>.] - ETA: 0s - loss: 0.0043\n",
      "Epoch 00065: reducing learning rate to 0.0024999999441206455.\n",
      "3000/3000 [==============================] - 296s - loss: 0.0043   \n",
      "Epoch 67/100\n",
      "2999/3000 [============================>.] - ETA: 0s - loss: 0.0043- - ET\n",
      "Epoch 00066: reducing learning rate to 0.0012499999720603228.\n",
      "3000/3000 [==============================] - 296s - loss: 0.0043   \n",
      "Epoch 68/100\n",
      "2999/3000 [============================>.] - ETA: 0s - loss: 0.0038\n",
      "Epoch 00067: reducing learning rate to 0.0006249999860301614.\n",
      "3000/3000 [==============================] - 296s - loss: 0.0038   \n",
      "Epoch 69/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0034   \n",
      "Epoch 70/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0033   \n",
      "Epoch 71/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0032   \n",
      "Epoch 72/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0031   \n",
      "Epoch 73/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0030   \n",
      "Epoch 74/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0030   \n",
      "Epoch 75/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0030   \n",
      "Epoch 76/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0029   \n",
      "Epoch 77/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0029   \n",
      "Epoch 78/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0029   \n",
      "Epoch 79/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0029   \n",
      "Epoch 80/100\n",
      "3000/3000 [==============================] - 300s - loss: 0.0028   \n",
      "Epoch 81/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0028   \n",
      "Epoch 82/100\n",
      "3000/3000 [==============================] - 299s - loss: 0.0028   \n",
      "Epoch 83/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0028   \n",
      "Epoch 84/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0028   \n",
      "Epoch 85/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0028   \n",
      "Epoch 86/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0028   - ETA: 0s - los\n",
      "Epoch 87/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 88/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0027   \n",
      "Epoch 89/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 90/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0027   \n",
      "Epoch 91/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 92/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 93/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 95/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0027   \n",
      "Epoch 96/100\n",
      "3000/3000 [==============================] - 298s - loss: 0.0027   \n",
      "Epoch 97/100\n",
      "3000/3000 [==============================] - 296s - loss: 0.0027   \n",
      "Epoch 98/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0026   \n",
      "Epoch 99/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0026   \n",
      "Epoch 100/100\n",
      "3000/3000 [==============================] - 297s - loss: 0.0026   \n"
     ]
    }
   ],
   "source": [
    "## Not using data augmentation.\n",
    "\n",
    "lrScheduling = keras.callbacks.ReduceLROnPlateau(monitor='loss',\n",
    "                                                 factor=0.5, patience=0, verbose=1, \n",
    "                                                 mode='auto', epsilon=0.0000001, \n",
    "                                                 cooldown=0, min_lr=0.0001)\n",
    "check_point = keras.callbacks.ModelCheckpoint(save_temp_model_name, \n",
    "                                              monitor='loss', verbose=0, \n",
    "                                              save_best_only=True, save_weights_only=False, \n",
    "                                              mode='auto', period=1)\n",
    "history = model.fit(train_X, train_Y,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          shuffle=True,\n",
    "          initial_epoch=epoch_to_start,\n",
    "          callbacks = [lrScheduling, check_point])\n",
    "    \n",
    "np.save(model_name + '.npy', history.history)\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
